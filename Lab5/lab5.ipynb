{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fc3fb7",
   "metadata": {},
   "source": [
    "# Лабораторная работа № 5\n",
    "\n",
    "## Распознавание объектов на фотографиях\n",
    "\n",
    "Задачи:\n",
    "\n",
    "   1. Ознакомиться со сверточными нейронными сетями\n",
    "   2. Изучить построение модели в Keras в функциональном виде\n",
    "   3. Изучить работу слоя разреживания (Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e581aba",
   "metadata": {},
   "source": [
    "# Цель работы:\n",
    "Распознавание объектов на фотографиях (Object Recognition in Photographs)\n",
    "CIFAR-10 (классификация небольших изображений по десяти классам: самолет,\n",
    "автомобиль, птица, кошка, олень, собака, лягушка, лошадь, корабль и грузовик)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad17c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acfd8f4-5f98-4f85-9862-41b4dddcfd11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 154s 1us/step\n",
      "170508288/170498071 [==============================] - 154s 1us/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 3 # использование ядра 3x3 \n",
    "pool_size = 2 #   использование объединения 2х2 \n",
    "conv_depth_1 = 32 #  32 ядра на слой преобразования\n",
    "conv_depth_2 = 64 # переход на 64 после первого уровня объединения\n",
    "drop_prob_1 = 0.25 #  вероятность 0,25\n",
    "drop_prob_2 = 0.5 #  в плотном слое вероятность 0,5\n",
    "hidden_size = 512 # в плотном слое 512 нейронов\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "num_train, depth, height, width = X_train.shape \n",
    "num_test = X_test.shape[0] \n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= np.max(X_train)\n",
    "X_test /= np.max(X_train) \n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) \n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) \n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # N.B. глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)  #Для предотвращения переобучения\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding - сохранять\n",
    "#размер исходного изображения, рисунок дополняется нулями по краям. \n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=0, validation_split=0.1) # \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8557f67-cb0d-4a0b-acc3-4fd5edf32e23",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 3 # использование ядра 3x3 \n",
    "pool_size = 2 # использование объединения 2х2 \n",
    "conv_depth_1 = 32 #  32 ядра на слой преобразования\n",
    "conv_depth_2 = 64 # переход на 64 после первого уровня объединения\n",
    "drop_prob_1 = 0.25 # выбывает после объединения с вероятностью 0,25   мы будем применять dropout после каждого слоя подвыборки, а также после полносвязного слоя;\n",
    "drop_prob_2 = 0.5 # выпадают в плотном слое с вероятностью 0,5\n",
    "hidden_size = 512 # в полносвязном  слое будет 512 нейронов \n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # извлечение данных CIFAR-10\n",
    "num_train, depth, height, width = X_train.shape # в CIFAR-10 содержится 50000 обучающих примеров\n",
    "num_test = X_test.shape[0] # в CIFAR-10 содержится 10000 тестовых примеров\n",
    "num_classes = np.unique(y_train).shape[0] # существует 10 классов изображений\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "X_test /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # кодирование тренировачных данных\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # кодирование тестировочных данных\n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # обратите внимание, глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "#drop_1 = Dropout(drop_prob_1)(pool_1)  #Регуляризация – это любая модификация алгоритма обучения, предпринятая с целью уменьшить его ошибку обобщения, не уменьшая ошибку обучения.\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(pool_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding='same' входное изображение должно иметь нулевое заполнение, чтобы вывод в свертке не отличался по размеру от ввода. \n",
    "#drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(pool_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "#drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(hidden)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1) # verbose=1 - индикатор выполнения\n",
    "#model.evaluate прогнозирует значения и вычисляет потери и все прикрепленные метрики к модели по заданному набору данных. \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1784fbd-7cda-4634-a4f2-db90333cac7c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 3 # мы будем использовать ядра 3x3 повсюду\n",
    "pool_size = 2 # мы будем использовать объединение 2х2 повсюду\n",
    "conv_depth_1 = 64 # первоначально у нас будет 32 ядра на слой преобразования\n",
    "conv_depth_2 = 128 # переключение на 64 после первого уровня объединения\n",
    "drop_prob_1 = 0.25 # выбывает после объединения с вероятностью 0,25\n",
    "drop_prob_2 = 0.5 # выпадают в плотном слое с вероятностью 0,5\n",
    "hidden_size = 512 # в плотном слое будет 512 нейронов\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # извлечение данных CIFAR-10\n",
    "num_train, depth, height, width = X_train.shape # в CIFAR-10 содержится 50000 обучающих примеров\n",
    "num_test = X_test.shape[0] # в CIFAR-10 содержится 10000 тестовых примеров\n",
    "num_classes = np.unique(y_train).shape[0] # существует 10 классов изображений\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "X_test /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # кодирование тренировачных данных\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # кодирование тестировочных данных\n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # N.B. глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding='same' входное изображение должно иметь нулевое заполнение, чтобы вывод в свертке не отличался по размеру от ввода. \n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1) # \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cda5b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 2 # мы будем использовать ядра 3x3 повсюду\n",
    "pool_size = 2 # мы будем использовать объединение 2х2 повсюду\n",
    "conv_depth_1 = 32 # первоначально у нас будет 32 ядра на слой преобразования\n",
    "conv_depth_2 = 64 # переключение на 64 после первого уровня объединения\n",
    "drop_prob_1 = 0.25 # выбывает после объединения с вероятностью 0,25\n",
    "drop_prob_2 = 0.5 # выпадают в плотном слое с вероятностью 0,5\n",
    "hidden_size = 512 # в плотном слое будет 512 нейронов\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # извлечение данных CIFAR-10\n",
    "num_train, depth, height, width = X_train.shape # в CIFAR-10 содержится 50000 обучающих примеров\n",
    "num_test = X_test.shape[0] # в CIFAR-10 содержится 10000 тестовых примеров\n",
    "num_classes = np.unique(y_train).shape[0] # существует 10 классов изображений\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "X_test /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # кодирование тренировачных данных\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # кодирование тестировочных данных\n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # N.B. глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding='same' входное изображение должно иметь нулевое заполнение, чтобы вывод в свертке не отличался по размеру от ввода. \n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1) # \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f452c0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_epochs = 200 \n",
    "kernel_size = 5 # мы будем использовать ядра 3x3 повсюду\n",
    "pool_size = 2 # мы будем использовать объединение 2х2 повсюду\n",
    "conv_depth_1 = 32 # первоначально у нас будет 32 ядра на слой преобразования\n",
    "conv_depth_2 = 64 # переключение на 64 после первого уровня объединения\n",
    "drop_prob_1 = 0.25 # выбывает после объединения с вероятностью 0,25\n",
    "drop_prob_2 = 0.5 # выпадают в плотном слое с вероятностью 0,5\n",
    "hidden_size = 512 # в плотном слое будет 512 нейронов\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # извлечение данных CIFAR-10\n",
    "num_train, depth, height, width = X_train.shape # в CIFAR-10 содержится 50000 обучающих примеров\n",
    "num_test = X_test.shape[0] # в CIFAR-10 содержится 10000 тестовых примеров\n",
    "num_classes = np.unique(y_train).shape[0] # существует 10 классов изображений\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "X_test /= np.max(X_train) # Нормализуйте данные до диапазона [0, 1]\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # кодирование тренировачных данных\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # кодирование тестировочных данных\n",
    "\n",
    "\n",
    "inp = Input(shape=(depth, height, width)) # N.B. глубина занимает первое место в Керасе\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(inp) #устарело\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size,\n",
    "padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size),padding='same')(conv_4) # padding='same' входное изображение должно иметь нулевое заполнение, чтобы вывод в свертке не отличался по размеру от ввода. \n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "#Теперь выровняйте до 1D, примените Плотный -> ReLU (с выпадением) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "model = Model(inputs=inp, outputs=out) # Чтобы определить модель, просто укажите ее входные и выходные слои\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1) # \n",
    "model.evaluate(X_test, Y_test, verbose=1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
