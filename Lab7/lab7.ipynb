{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643ee474-87fe-418c-91eb-fba8d5b4cee8",
   "metadata": {},
   "source": [
    "## Лабораторная работа No 7\n",
    "### Классификация обзоров фильмов\n",
    "## Цель\n",
    "Классификация последовательностей - это проблема прогнозирующего моделирования, когда у вас есть некоторая последовательность входных данных в пространстве или времени, и задача состоит в том, чтобы предсказать категорию для последовательности. Проблема усложняется тем, что последовательности могут различаться по длине, состоять из очень большого словарного запаса входных символов и могут потребовать от модели изучения долгосрочного контекста или зависимостей между символами во входной последовательности. В данной лабораторной работе также будет использоваться датасет IMDb, однако обучение будет проводиться с помощью рекуррентной нейронной сети.\n",
    "\n",
    "## Задачи\n",
    "1. Ознакомиться с рекуррентными нейронными сетями\n",
    "2. Изучить способы классификации текста\n",
    "3. Ознакомиться с ансамблированием сетей\n",
    "4. Построить ансамбль сетей, который позволит получать точность не менее 97%\n",
    "\n",
    "# Требования\n",
    "1. Найти набор оптимальных ИНС для классификации текста\n",
    "2. Провести ансамблирование моделей\n",
    "3. Написать функцию/функции, которые позволят загружать текст и получать результат ансамбля сетей\n",
    "4. Провести тестирование сетей на своих текстах (привести в отчете)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd70ee-d83b-4eb9-ae95-70e470419cf9",
   "metadata": {},
   "source": [
    "# Выполнение работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e62a1a1-c298-412b-b32e-9f7bf6a738f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import tkinter as tk\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3776971c-e3a7-4dba-ba93-36f16cfe0e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 640s 2s/step - loss: 0.4524 - accuracy: 0.7823 - val_loss: 0.3341 - val_accuracy: 0.8650\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 511s 1s/step - loss: 0.3089 - accuracy: 0.8753 - val_loss: 0.3112 - val_accuracy: 0.8729\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.2586 - accuracy: 0.9002 - val_loss: 0.3411 - val_accuracy: 0.8579\n",
      "Accuracy: 85.79%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b2e92-28c3-4c0a-91ab-a629066dc45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 250, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 320s 791ms/step - loss: 0.4313 - accuracy: 0.7841 - val_loss: 0.3105 - val_accuracy: 0.8767\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 417s 1s/step - loss: 0.2441 - accuracy: 0.9054 - val_loss: 0.2866 - val_accuracy: 0.8796\n",
      "Epoch 3/3\n",
      "149/391 [==========>...................] - ETA: 3:11 - loss: 0.1949 - accuracy: 0.9259"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data,\n",
    "testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "\n",
    "targets = np.concatenate((training_targets, testing_targets),\n",
    "axis=0)\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length,\n",
    "input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) #filters - Количество выходных фильтров в свертке\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a8306-7e25-42f9-9075-d90e3c7bccf0",
   "metadata": {},
   "source": [
    "## Применение слоёв Dropout в архитектуре нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81a3a9-31ea-4f82-99ec-63b2813551d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data,\n",
    "testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "\n",
    "targets = np.concatenate((training_targets, testing_targets),\n",
    "axis=0)\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) #filters - Количество выходных фильтров в свертке\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(LSTM(100))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4, batch_size=32)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bedd0-fafa-4045-83fe-624dca0d11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data,\n",
    "testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "\n",
    "targets = np.concatenate((training_targets, testing_targets),\n",
    "axis=0)\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) #filters - Количество выходных фильтров в свертке\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(LSTM(100))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653fd6c-addb-474d-9915-fdf346e37cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb #Загружаем датесет IMDb,\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "\n",
    "top_words = 5000\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=top_words)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "X_test = data[:250]\n",
    "y_test = targets[:250]\n",
    "X_train = data[1:]\n",
    "y_train = targets[1:]\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32 # длина вектора, в котором будут представляться слова\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length)) # плотный вектор\n",
    "# input_length - размер входных данных\n",
    "model.add(LSTM(100)) # рекуррентный слой\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689d8de-49c8-4ffd-9854-24f4adaa8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb #Загружаем датесет IMDb,\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "\n",
    "top_words = 5000\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=top_words)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "X_test = data[:270] \n",
    "y_test = targets[:270]\n",
    "X_train = data[1:]\n",
    "y_train = targets[1:]\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32 #на вход получает номера слов, а на выходе выдаёт их векторные представления \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) # сверточный слой\n",
    "model.add(MaxPooling1D(pool_size=2)) # субдискретизирующий слой\n",
    "model.add(LSTM(100)) # рекуррентный слой\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb70597-5c46-4906-ae51-934b35638228",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = imdb.get_word_index()\n",
    "def predict(txt:str):\n",
    "    txt = txt.lower()\n",
    "    txt1 = \"\"\n",
    "    for i in txt:\n",
    "        if('a'<=i<='z' or i==' '):\n",
    "            txt1+=i\n",
    "    txt1=txt1.split()\n",
    "    tokens=np.array([min(index.get(i, 5000),5000)+3 for i in txt1])\n",
    "    vector = sequence.pad_sequences([tokens], maxlen=max_review_length)\n",
    "    p=model.predict(vector)\n",
    "    return \"POSITIVE \"+str(p) if p>0.5 else \"NEGATIVE \"+str(1-p)\n",
    "\n",
    "predict(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacec871-9244-4795-80c3-7d10858824d0",
   "metadata": {},
   "source": [
    "## POSITIVE\n",
    "This film is very well shot. I liked the acting. In general, the film is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87d638-f44e-4467-9999-879c057832ef",
   "metadata": {},
   "source": [
    "## NEGATIVE\n",
    "I really wanted to watch this movie because of the actors who played in it. But after watching this movie, I was very disappointed. The plot seemed to me very uninteresting and boring. In general, we can say that I did not like the film."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
